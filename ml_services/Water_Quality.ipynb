{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5ee546ac",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'flask'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mensemble\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m RandomForestRegressor\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpreprocessing\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m StandardScaler\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mflask\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Flask, request, jsonify\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mjoblib\u001b[39;00m\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'flask'"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os, math, warnings\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "\n",
    "# Try optional libraries\n",
    "try:\n",
    "    import xgboost as xgb\n",
    "    HAS_XGB = True\n",
    "except:\n",
    "    HAS_XGB = False\n",
    "\n",
    "try:\n",
    "    import tensorflow as tf\n",
    "    from tensorflow import keras\n",
    "    from tensorflow.keras import layers\n",
    "    HAS_TF = True\n",
    "except:\n",
    "    HAS_TF = False\n",
    "\n",
    "\n",
    "# =============== Synthetic Dataset ===================\n",
    "def seasonal(n, period=12, amplitude=1.0, phase=0.0):\n",
    "    t = np.arange(n)\n",
    "    return amplitude * np.sin(2*np.pi*(t/period) + phase)\n",
    "\n",
    "def trend(n, slope=0.0, start=0.0):\n",
    "    return start + slope*np.arange(n)\n",
    "\n",
    "def generate_data(n_months=900):\n",
    "    np.random.seed(42)\n",
    "    dates = pd.date_range(\"2010-01-01\", periods=n_months, freq=\"MS\")\n",
    "\n",
    "    temp = 20 + seasonal(n_months, 12, 5, 0.5) + np.random.normal(0, 1.2, n_months)\n",
    "    turbidity = np.clip(5 + 3*seasonal(n_months, 6) + np.random.normal(0, 1.4, n_months) + 0.02*trend(n_months, 0.05), 0, None)\n",
    "    ph = np.clip(7.2 + 0.2*seasonal(n_months, 12, 1.0, 1.0) + np.random.normal(0, 0.08, n_months), 6.2, 8.8)\n",
    "    do = np.clip(8.5 - 0.25*(temp-20) + np.random.normal(0, 0.4, n_months), 2.5, 14.0)\n",
    "    bod = np.clip(2.5 + 0.5*seasonal(n_months, 12, 1.0, 2.0) + np.random.normal(0, 0.4, n_months) + 0.02*turbidity, 0.5, None)\n",
    "    cod = np.clip(10 + 2.2*seasonal(n_months, 12, 1.0, 2.5) + np.random.normal(0, 1.6, n_months) + 1.5*bod, 2, None)\n",
    "    nitrate = np.clip(2.0 + 0.8*seasonal(n_months, 12, 1.0, 0.2) + np.random.normal(0, 0.3, n_months) + 0.03*turbidity, 0, None)\n",
    "    phosphate = np.clip(0.4 + 0.15*seasonal(n_months, 12, 1.0, 0.8) + np.random.normal(0, 0.05, n_months) + 0.01*turbidity, 0, None)\n",
    "    tds = np.clip(150 + 40*seasonal(n_months, 12, 1.0, 1.4) + np.random.normal(0, 20, n_months) + 3.0*temp, 50, None)\n",
    "    conductivity = np.clip(300 + 1.5*tds + np.random.normal(0, 30, n_months), 100, None)\n",
    "    fecal_coliform = np.clip(30 + 8*seasonal(n_months, 12, 1.0, 2.1) + np.random.normal(0, 6, n_months) + 1.5*turbidity, 0, None)\n",
    "\n",
    "    df = pd.DataFrame({\n",
    "        \"date\": dates,\n",
    "        \"temperature_C\": temp,\n",
    "        \"turbidity_NTU\": turbidity,\n",
    "        \"pH\": ph,\n",
    "        \"DO_mg_L\": do,\n",
    "        \"BOD_mg_L\": bod,\n",
    "        \"COD_mg_L\": cod,\n",
    "        \"nitrate_mg_L\": nitrate,\n",
    "        \"phosphate_mg_L\": phosphate,\n",
    "        \"TDS_mg_L\": tds,\n",
    "        \"conductivity_uS_cm\": conductivity,\n",
    "        \"fecal_coliform_CFU_100mL\": fecal_coliform\n",
    "    })\n",
    "\n",
    "    wqi = (\n",
    "        12 +\n",
    "        8*np.clip(1 - np.abs(df[\"pH\"]-7.0)/1.5, 0, 1) +\n",
    "        15*np.clip(df[\"DO_mg_L\"]/12, 0, 1) -\n",
    "        8*np.tanh(df[\"turbidity_NTU\"]/15) -\n",
    "        10*np.tanh(df[\"BOD_mg_L\"]/6) -\n",
    "        10*np.tanh(df[\"COD_mg_L\"]/40) -\n",
    "        6*np.tanh(df[\"nitrate_mg_L\"]/6) -\n",
    "        6*np.tanh(df[\"phosphate_mg_L\"]/0.8) -\n",
    "        8*np.tanh(df[\"fecal_coliform_CFU_100mL\"]/150) -\n",
    "        5*np.tanh(df[\"TDS_mg_L\"]/600)\n",
    "    )\n",
    "    wqi = np.clip(wqi + np.random.normal(0, 2.0, n_months), 0, 100)\n",
    "    df[\"WQI\"] = wqi\n",
    "    return df\n",
    "\n",
    "# Generate and save\n",
    "df = generate_data()\n",
    "os.makedirs(\"artifacts\", exist_ok=True)\n",
    "df.to_csv(\"artifacts/water_quality_synthetic.csv\", index=False)\n",
    "df.to_excel(\"artifacts/water_quality_synthetic.xlsx\", index=False)\n",
    "\n",
    "# =============== Train/Test Split ===================\n",
    "features = [c for c in df.columns if c not in [\"date\",\"WQI\"]]\n",
    "X = df[features].values\n",
    "y = df[\"WQI\"].values\n",
    "split_idx = int(0.8*len(df))\n",
    "X_train, y_train = X[:split_idx], y[:split_idx]\n",
    "X_test, y_test = X[split_idx:], y[split_idx:]\n",
    "\n",
    "# =============== Random Forest ===================\n",
    "rf = RandomForestRegressor(n_estimators=300, random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "rf_pred = rf.predict(X_test)\n",
    "\n",
    "def evaluate(name, y_true, y_pred):\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    rmse = math.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    print(f\"{name}: MAE={mae:.3f}, RMSE={rmse:.3f}, R²={r2:.3f}\")\n",
    "\n",
    "evaluate(\"Random Forest\", y_test, rf_pred)\n",
    "predicted_wqi = rf_pred\n",
    "\n",
    "def wqi_label(wqi):\n",
    "    if wqi >= 75:\n",
    "        return \"Good\"\n",
    "    elif wqi >= 50:\n",
    "        return \"Moderate\"\n",
    "    else:\n",
    "        return \"Poor\"\n",
    "        \n",
    "labels = [wqi_label(w) for w in predicted_wqi]\n",
    "\n",
    "\n",
    "\n",
    "# # =============== XGBoost ===================\n",
    "# if HAS_XGB:\n",
    "#     xgb_model = xgb.XGBRegressor(n_estimators=400, learning_rate=0.05, max_depth=5)\n",
    "#     xgb_model.fit(X_train, y_train)\n",
    "#     xgb_pred = xgb_model.predict(X_test)\n",
    "#     evaluate(\"XGBoost\", y_test, xgb_pred)\n",
    "\n",
    "# # =============== LSTM ===================\n",
    "# if HAS_TF:\n",
    "#     scaler = StandardScaler()\n",
    "#     X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "#     def make_sequences(X, y, window=12):\n",
    "#         Xs, ys = [], []\n",
    "#         for i in range(len(X)-window+1):\n",
    "#             Xs.append(X[i:i+window])\n",
    "#             ys.append(y[i+window-1])\n",
    "#         return np.array(Xs), np.array(ys)\n",
    "\n",
    "#     X_seq, y_seq = make_sequences(X_scaled, y, window=12)\n",
    "#     seq_split = int(0.8*len(X_seq))\n",
    "#     X_seq_train, y_seq_train = X_seq[:seq_split], y_seq[:seq_split]\n",
    "#     X_seq_test, y_seq_test = X_seq[seq_split:], y_seq[seq_split:]\n",
    "\n",
    "#     model = keras.Sequential([\n",
    "#         layers.LSTM(64, return_sequences=True, input_shape=(12, X_seq.shape[2])),\n",
    "#         layers.Dropout(0.2),\n",
    "#         layers.LSTM(32),\n",
    "#         layers.Dense(16, activation=\"relu\"),\n",
    "#         layers.Dense(1)\n",
    "#     ])\n",
    "#     model.compile(optimizer=\"adam\", loss=\"mse\")\n",
    "#     model.fit(X_seq_train, y_seq_train, epochs=10, batch_size=32, verbose=0)\n",
    "#     lstm_pred = model.predict(X_seq_test).ravel()\n",
    "#     evaluate(\"LSTM\", y_seq_test, lstm_pred)\n",
    "\n",
    "print(\"\\nDataset saved to 'artifacts/water_quality_synthetic.csv' and '.xlsx'\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b76de02d-ee2e-406e-9ac9-a37eaa3baac1",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1501236008.py, line 1)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mpip install flask,joblib\u001b[39m\n        ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "pip install flask,joblib\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "abf59d2a-df3f-4167-aa61-bacafc6871e5",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (4007692362.py, line 1)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mpip3 install pandas\u001b[39m\n         ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "pip3 install pandas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "33b71acb-9055-4ef7-a876-381057827d8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Matplotlib is building the font cache; this may take a moment.\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'openpyxl'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 87\u001b[39m\n\u001b[32m     85\u001b[39m os.makedirs(\u001b[33m\"\u001b[39m\u001b[33martifacts\u001b[39m\u001b[33m\"\u001b[39m, exist_ok=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     86\u001b[39m df.to_csv(\u001b[33m\"\u001b[39m\u001b[33martifacts/water_quality_synthetic.csv\u001b[39m\u001b[33m\"\u001b[39m, index=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m---> \u001b[39m\u001b[32m87\u001b[39m \u001b[43mdf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto_excel\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43martifacts/water_quality_synthetic.xlsx\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     89\u001b[39m \u001b[38;5;66;03m# =============== Train/Test Split ===================\u001b[39;00m\n\u001b[32m     90\u001b[39m features = [c \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m df.columns \u001b[38;5;28;01mif\u001b[39;00m c \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m [\u001b[33m\"\u001b[39m\u001b[33mdate\u001b[39m\u001b[33m\"\u001b[39m,\u001b[33m\"\u001b[39m\u001b[33mWQI\u001b[39m\u001b[33m\"\u001b[39m]]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/pandas/util/_decorators.py:333\u001b[39m, in \u001b[36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    327\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) > num_allow_args:\n\u001b[32m    328\u001b[39m     warnings.warn(\n\u001b[32m    329\u001b[39m         msg.format(arguments=_format_argument_list(allow_args)),\n\u001b[32m    330\u001b[39m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[32m    331\u001b[39m         stacklevel=find_stack_level(),\n\u001b[32m    332\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m333\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/pandas/core/generic.py:2439\u001b[39m, in \u001b[36mNDFrame.to_excel\u001b[39m\u001b[34m(self, excel_writer, sheet_name, na_rep, float_format, columns, header, index, index_label, startrow, startcol, engine, merge_cells, inf_rep, freeze_panes, storage_options, engine_kwargs)\u001b[39m\n\u001b[32m   2426\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mio\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mformats\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mexcel\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ExcelFormatter\n\u001b[32m   2428\u001b[39m formatter = ExcelFormatter(\n\u001b[32m   2429\u001b[39m     df,\n\u001b[32m   2430\u001b[39m     na_rep=na_rep,\n\u001b[32m   (...)\u001b[39m\u001b[32m   2437\u001b[39m     inf_rep=inf_rep,\n\u001b[32m   2438\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m2439\u001b[39m \u001b[43mformatter\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2440\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexcel_writer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2441\u001b[39m \u001b[43m    \u001b[49m\u001b[43msheet_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43msheet_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2442\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstartrow\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstartrow\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2443\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstartcol\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstartcol\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2444\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfreeze_panes\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfreeze_panes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2445\u001b[39m \u001b[43m    \u001b[49m\u001b[43mengine\u001b[49m\u001b[43m=\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2446\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2447\u001b[39m \u001b[43m    \u001b[49m\u001b[43mengine_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mengine_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2448\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/pandas/io/formats/excel.py:943\u001b[39m, in \u001b[36mExcelFormatter.write\u001b[39m\u001b[34m(self, writer, sheet_name, startrow, startcol, freeze_panes, engine, storage_options, engine_kwargs)\u001b[39m\n\u001b[32m    941\u001b[39m     need_save = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    942\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m943\u001b[39m     writer = \u001b[43mExcelWriter\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    944\u001b[39m \u001b[43m        \u001b[49m\u001b[43mwriter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    945\u001b[39m \u001b[43m        \u001b[49m\u001b[43mengine\u001b[49m\u001b[43m=\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    946\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    947\u001b[39m \u001b[43m        \u001b[49m\u001b[43mengine_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mengine_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    948\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    949\u001b[39m     need_save = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    951\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/pandas/io/excel/_openpyxl.py:57\u001b[39m, in \u001b[36mOpenpyxlWriter.__init__\u001b[39m\u001b[34m(self, path, engine, date_format, datetime_format, mode, storage_options, if_sheet_exists, engine_kwargs, **kwargs)\u001b[39m\n\u001b[32m     44\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\n\u001b[32m     45\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m     46\u001b[39m     path: FilePath | WriteExcelBuffer | ExcelWriter,\n\u001b[32m   (...)\u001b[39m\u001b[32m     55\u001b[39m ) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     56\u001b[39m     \u001b[38;5;66;03m# Use the openpyxl module as the Excel writer.\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m57\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mopenpyxl\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mworkbook\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Workbook\n\u001b[32m     59\u001b[39m     engine_kwargs = combine_kwargs(engine_kwargs, kwargs)\n\u001b[32m     61\u001b[39m     \u001b[38;5;28msuper\u001b[39m().\u001b[34m__init__\u001b[39m(\n\u001b[32m     62\u001b[39m         path,\n\u001b[32m     63\u001b[39m         mode=mode,\n\u001b[32m   (...)\u001b[39m\u001b[32m     66\u001b[39m         engine_kwargs=engine_kwargs,\n\u001b[32m     67\u001b[39m     )\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'openpyxl'"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os, math, warnings\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Try optional libraries\n",
    "try:\n",
    "    import xgboost as xgb\n",
    "    HAS_XGB = True\n",
    "except:\n",
    "    HAS_XGB = False\n",
    "\n",
    "try:\n",
    "    import tensorflow as tf\n",
    "    from tensorflow import keras\n",
    "    from tensorflow.keras import layers\n",
    "    HAS_TF = True\n",
    "except:\n",
    "    HAS_TF = False\n",
    "\n",
    "\n",
    "# =============== Synthetic Dataset ===================\n",
    "def seasonal(n, period=12, amplitude=1.0, phase=0.0):\n",
    "    t = np.arange(n)\n",
    "    return amplitude * np.sin(2*np.pi*(t/period) + phase)\n",
    "\n",
    "def trend(n, slope=0.0, start=0.0):\n",
    "    return start + slope*np.arange(n)\n",
    "\n",
    "def generate_data(n_months=900):\n",
    "    np.random.seed(42)\n",
    "    dates = pd.date_range(\"2010-01-01\", periods=n_months, freq=\"MS\")\n",
    "\n",
    "    temp = 20 + seasonal(n_months, 12, 5, 0.5) + np.random.normal(0, 1.2, n_months)\n",
    "    turbidity = np.clip(5 + 3*seasonal(n_months, 6) + np.random.normal(0, 1.4, n_months) + 0.02*trend(n_months, 0.05), 0, None)\n",
    "    ph = np.clip(7.2 + 0.2*seasonal(n_months, 12, 1.0, 1.0) + np.random.normal(0, 0.08, n_months), 6.2, 8.8)\n",
    "    do = np.clip(8.5 - 0.25*(temp-20) + np.random.normal(0, 0.4, n_months), 2.5, 14.0)\n",
    "    bod = np.clip(2.5 + 0.5*seasonal(n_months, 12, 1.0, 2.0) + np.random.normal(0, 0.4, n_months) + 0.02*turbidity, 0.5, None)\n",
    "    cod = np.clip(10 + 2.2*seasonal(n_months, 12, 1.0, 2.5) + np.random.normal(0, 1.6, n_months) + 1.5*bod, 2, None)\n",
    "    nitrate = np.clip(2.0 + 0.8*seasonal(n_months, 12, 1.0, 0.2) + np.random.normal(0, 0.3, n_months) + 0.03*turbidity, 0, None)\n",
    "    phosphate = np.clip(0.4 + 0.15*seasonal(n_months, 12, 1.0, 0.8) + np.random.normal(0, 0.05, n_months) + 0.01*turbidity, 0, None)\n",
    "    tds = np.clip(150 + 40*seasonal(n_months, 12, 1.0, 1.4) + np.random.normal(0, 20, n_months) + 3.0*temp, 50, None)\n",
    "    conductivity = np.clip(300 + 1.5*tds + np.random.normal(0, 30, n_months), 100, None)\n",
    "    fecal_coliform = np.clip(30 + 8*seasonal(n_months, 12, 1.0, 2.1) + np.random.normal(0, 6, n_months) + 1.5*turbidity, 0, None)\n",
    "\n",
    "    df = pd.DataFrame({\n",
    "        \"date\": dates,\n",
    "        \"temperature_C\": temp,\n",
    "        \"turbidity_NTU\": turbidity,\n",
    "        \"pH\": ph,\n",
    "        \"DO_mg_L\": do,\n",
    "        \"BOD_mg_L\": bod,\n",
    "        \"COD_mg_L\": cod,\n",
    "        \"nitrate_mg_L\": nitrate,\n",
    "        \"phosphate_mg_L\": phosphate,\n",
    "        \"TDS_mg_L\": tds,\n",
    "        \"conductivity_uS_cm\": conductivity,\n",
    "        \"fecal_coliform_CFU_100mL\": fecal_coliform\n",
    "    })\n",
    "\n",
    "    wqi = (\n",
    "        12 +\n",
    "        8*np.clip(1 - np.abs(df[\"pH\"]-7.0)/1.5, 0, 1) +\n",
    "        15*np.clip(df[\"DO_mg_L\"]/12, 0, 1) -\n",
    "        8*np.tanh(df[\"turbidity_NTU\"]/15) -\n",
    "        10*np.tanh(df[\"BOD_mg_L\"]/6) -\n",
    "        10*np.tanh(df[\"COD_mg_L\"]/40) -\n",
    "        6*np.tanh(df[\"nitrate_mg_L\"]/6) -\n",
    "        6*np.tanh(df[\"phosphate_mg_L\"]/0.8) -\n",
    "        8*np.tanh(df[\"fecal_coliform_CFU_100mL\"]/150) -\n",
    "        5*np.tanh(df[\"TDS_mg_L\"]/600)\n",
    "    )\n",
    "    wqi = np.clip(wqi + np.random.normal(0, 2.0, n_months), 0, 100)\n",
    "    df[\"WQI\"] = wqi\n",
    "    return df\n",
    "\n",
    "# Generate and save\n",
    "df = generate_data()\n",
    "os.makedirs(\"artifacts\", exist_ok=True)\n",
    "df.to_csv(\"artifacts/water_quality_synthetic.csv\", index=False)\n",
    "df.to_excel(\"artifacts/water_quality_synthetic.xlsx\", index=False)\n",
    "\n",
    "# =============== Train/Test Split ===================\n",
    "features = [c for c in df.columns if c not in [\"date\",\"WQI\"]]\n",
    "X = df[features].values\n",
    "y = df[\"WQI\"].values\n",
    "split_idx = int(0.8*len(df))\n",
    "X_train, y_train = X[:split_idx], y[:split_idx]\n",
    "X_test, y_test = X[split_idx:], y[split_idx:]\n",
    "\n",
    "# =============== Random Forest ===================\n",
    "rf = RandomForestRegressor(n_estimators=300, random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "rf_pred = rf.predict(X_test)\n",
    "\n",
    "def evaluate(name, y_true, y_pred):\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    rmse = math.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    print(f\"{name}: MAE={mae:.3f}, RMSE={rmse:.3f}, R²={r2:.3f}\")\n",
    "\n",
    "evaluate(\"Random Forest\", y_test, rf_pred)\n",
    "\n",
    "def wqi_label(wqi):\n",
    "    if wqi >= 75:\n",
    "        return \"Good\"\n",
    "    elif wqi >= 50:\n",
    "        return \"Moderate\"\n",
    "    else:\n",
    "        return \"Poor\"\n",
    "\n",
    "\n",
    "# # =============== XGBoost ===================\n",
    "# if HAS_XGB:\n",
    "#     xgb_model = xgb.XGBRegressor(n_estimators=400, learning_rate=0.05, max_depth=5)\n",
    "#     xgb_model.fit(X_train, y_train)\n",
    "#     xgb_pred = xgb_model.predict(X_test)\n",
    "#     evaluate(\"XGBoost\", y_test, xgb_pred)\n",
    "\n",
    "# # =============== LSTM ===================\n",
    "# if HAS_TF:\n",
    "#     scaler = StandardScaler()\n",
    "#     X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "#     def make_sequences(X, y, window=12):\n",
    "#         Xs, ys = [], []\n",
    "#         for i in range(len(X)-window+1):\n",
    "#             Xs.append(X[i:i+window])\n",
    "#             ys.append(y[i+window-1])\n",
    "#         return np.array(Xs), np.array(ys)\n",
    "\n",
    "#     X_seq, y_seq = make_sequences(X_scaled, y, window=12)\n",
    "#     seq_split = int(0.8*len(X_seq))\n",
    "#     X_seq_train, y_seq_train = X_seq[:seq_split], y_seq[:seq_split]\n",
    "#     X_seq_test, y_seq_test = X_seq[seq_split:], y_seq[seq_split:]\n",
    "\n",
    "#     model = keras.Sequential([\n",
    "#         layers.LSTM(64, return_sequences=True, input_shape=(12, X_seq.shape[2])),\n",
    "#         layers.Dropout(0.2),\n",
    "#         layers.LSTM(32),\n",
    "#         layers.Dense(16, activation=\"relu\"),\n",
    "#         layers.Dense(1)\n",
    "#     ])\n",
    "#     model.compile(optimizer=\"adam\", loss=\"mse\")\n",
    "#     model.fit(X_seq_train, y_seq_train, epochs=10, batch_size=32, verbose=0)\n",
    "#     lstm_pred = model.predict(X_seq_test).ravel()\n",
    "#     evaluate(\"LSTM\", y_seq_test, lstm_pred)\n",
    "\n",
    "print(\"\\nDataset saved to 'artifacts/water_quality_synthetic.csv' and '.xlsx'\")\n",
    "\n",
    "label = wqi_label(predicted_wqi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "be31f3e7-f021-4f8a-b37e-77d109b2e999",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting matplotlib\n",
      "  Downloading matplotlib-3.10.8-cp313-cp313-macosx_11_0_arm64.whl.metadata (52 kB)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib)\n",
      "  Downloading contourpy-1.3.3-cp313-cp313-macosx_11_0_arm64.whl.metadata (5.5 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib)\n",
      "  Downloading cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib)\n",
      "  Downloading fonttools-4.61.1-cp313-cp313-macosx_10_13_universal2.whl.metadata (114 kB)\n",
      "Collecting kiwisolver>=1.3.1 (from matplotlib)\n",
      "  Downloading kiwisolver-1.4.9-cp313-cp313-macosx_11_0_arm64.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: numpy>=1.23 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from matplotlib) (2.4.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from matplotlib) (25.0)\n",
      "Collecting pillow>=8 (from matplotlib)\n",
      "  Downloading pillow-12.0.0-cp313-cp313-macosx_11_0_arm64.whl.metadata (8.8 kB)\n",
      "Collecting pyparsing>=3 (from matplotlib)\n",
      "  Downloading pyparsing-3.3.1-py3-none-any.whl.metadata (5.6 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
      "Downloading matplotlib-3.10.8-cp313-cp313-macosx_11_0_arm64.whl (8.1 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.1/8.1 MB\u001b[0m \u001b[31m34.5 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m8.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading contourpy-1.3.3-cp313-cp313-macosx_11_0_arm64.whl (274 kB)\n",
      "Downloading cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Downloading fonttools-4.61.1-cp313-cp313-macosx_10_13_universal2.whl (2.8 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m35.1 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading kiwisolver-1.4.9-cp313-cp313-macosx_11_0_arm64.whl (64 kB)\n",
      "Downloading pillow-12.0.0-cp313-cp313-macosx_11_0_arm64.whl (4.7 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.7/4.7 MB\u001b[0m \u001b[31m37.0 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pyparsing-3.3.1-py3-none-any.whl (121 kB)\n",
      "Installing collected packages: pyparsing, pillow, kiwisolver, fonttools, cycler, contourpy, matplotlib\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7/7\u001b[0m [matplotlib]\u001b[0m \u001b[32m6/7\u001b[0m [matplotlib]ow]\n",
      "\u001b[1A\u001b[2KSuccessfully installed contourpy-1.3.3 cycler-0.12.1 fonttools-4.61.1 kiwisolver-1.4.9 matplotlib-3.10.8 pillow-12.0.0 pyparsing-3.3.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install matplotlib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "631dc79e-e513-4624-8539-ce5265503397",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sklearn\n",
      "  Downloading sklearn-0.0.post12.tar.gz (2.6 kB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25lerror\n",
      "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "  \n",
      "  \u001b[31m×\u001b[0m \u001b[32mGetting requirements to build wheel\u001b[0m did not run successfully.\n",
      "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
      "  \u001b[31m╰─>\u001b[0m \u001b[31m[15 lines of output]\u001b[0m\n",
      "  \u001b[31m   \u001b[0m The 'sklearn' PyPI package is deprecated, use 'scikit-learn'\n",
      "  \u001b[31m   \u001b[0m rather than 'sklearn' for pip commands.\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m Here is how to fix this error in the main use cases:\n",
      "  \u001b[31m   \u001b[0m - use 'pip install scikit-learn' rather than 'pip install sklearn'\n",
      "  \u001b[31m   \u001b[0m - replace 'sklearn' by 'scikit-learn' in your pip requirements files\n",
      "  \u001b[31m   \u001b[0m   (requirements.txt, setup.py, setup.cfg, Pipfile, etc ...)\n",
      "  \u001b[31m   \u001b[0m - if the 'sklearn' package is used by one of your dependencies,\n",
      "  \u001b[31m   \u001b[0m   it would be great if you take some time to track which package uses\n",
      "  \u001b[31m   \u001b[0m   'sklearn' instead of 'scikit-learn' and report it to their issue tracker\n",
      "  \u001b[31m   \u001b[0m - as a last resort, set the environment variable\n",
      "  \u001b[31m   \u001b[0m   SKLEARN_ALLOW_DEPRECATED_SKLEARN_PACKAGE_INSTALL=True to avoid this error\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m More information is available at\n",
      "  \u001b[31m   \u001b[0m https://github.com/scikit-learn/sklearn-pypi-package\n",
      "  \u001b[31m   \u001b[0m \u001b[31m[end of output]\u001b[0m\n",
      "  \n",
      "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "\u001b[31mERROR: Failed to build 'sklearn' when getting requirements to build wheel\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[?25hNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "pip install sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5cfd0a2f-572b-47ec-891f-0596c424dea2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sklearn\n",
      "  Using cached sklearn-0.0.post12.tar.gz (2.6 kB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25lerror\n",
      "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "  \n",
      "  \u001b[31m×\u001b[0m \u001b[32mGetting requirements to build wheel\u001b[0m did not run successfully.\n",
      "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
      "  \u001b[31m╰─>\u001b[0m \u001b[31m[15 lines of output]\u001b[0m\n",
      "  \u001b[31m   \u001b[0m The 'sklearn' PyPI package is deprecated, use 'scikit-learn'\n",
      "  \u001b[31m   \u001b[0m rather than 'sklearn' for pip commands.\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m Here is how to fix this error in the main use cases:\n",
      "  \u001b[31m   \u001b[0m - use 'pip install scikit-learn' rather than 'pip install sklearn'\n",
      "  \u001b[31m   \u001b[0m - replace 'sklearn' by 'scikit-learn' in your pip requirements files\n",
      "  \u001b[31m   \u001b[0m   (requirements.txt, setup.py, setup.cfg, Pipfile, etc ...)\n",
      "  \u001b[31m   \u001b[0m - if the 'sklearn' package is used by one of your dependencies,\n",
      "  \u001b[31m   \u001b[0m   it would be great if you take some time to track which package uses\n",
      "  \u001b[31m   \u001b[0m   'sklearn' instead of 'scikit-learn' and report it to their issue tracker\n",
      "  \u001b[31m   \u001b[0m - as a last resort, set the environment variable\n",
      "  \u001b[31m   \u001b[0m   SKLEARN_ALLOW_DEPRECATED_SKLEARN_PACKAGE_INSTALL=True to avoid this error\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m More information is available at\n",
      "  \u001b[31m   \u001b[0m https://github.com/scikit-learn/sklearn-pypi-package\n",
      "  \u001b[31m   \u001b[0m \u001b[31m[end of output]\u001b[0m\n",
      "  \n",
      "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "\u001b[31mERROR: Failed to build 'sklearn' when getting requirements to build wheel\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[?25hNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install sklearn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ffa589ff-2b50-4eb1-93c3-c16011ee1058",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.8.0-cp313-cp313-macosx_12_0_arm64.whl.metadata (11 kB)\n",
      "Requirement already satisfied: numpy>=1.24.1 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from scikit-learn) (2.4.0)\n",
      "Collecting scipy>=1.10.0 (from scikit-learn)\n",
      "  Downloading scipy-1.16.3-cp313-cp313-macosx_14_0_arm64.whl.metadata (62 kB)\n",
      "Collecting joblib>=1.3.0 (from scikit-learn)\n",
      "  Downloading joblib-1.5.3-py3-none-any.whl.metadata (5.5 kB)\n",
      "Collecting threadpoolctl>=3.2.0 (from scikit-learn)\n",
      "  Downloading threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
      "Downloading scikit_learn-1.8.0-cp313-cp313-macosx_12_0_arm64.whl (8.0 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.0/8.0 MB\u001b[0m \u001b[31m38.0 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading joblib-1.5.3-py3-none-any.whl (309 kB)\n",
      "Downloading scipy-1.16.3-cp313-cp313-macosx_14_0_arm64.whl (20.9 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.9/20.9 MB\u001b[0m \u001b[31m33.4 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\u001b[0m eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: threadpoolctl, scipy, joblib, scikit-learn\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4/4\u001b[0m [scikit-learn]0m \u001b[32m3/4\u001b[0m [scikit-learn]\n",
      "\u001b[1A\u001b[2KSuccessfully installed joblib-1.5.3 scikit-learn-1.8.0 scipy-1.16.3 threadpoolctl-3.6.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install scikit-learn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "464e84a4-0544-457c-a0c5-28807a5f3422",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (System)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
